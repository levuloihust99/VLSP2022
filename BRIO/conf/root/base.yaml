# @package _global_
config: base

batch_size: 1 # batch size on one gpu, one step
epoch: 100
report_freq: 100 # report frequency
accumulate_step: 32) # accumulate gradients steps
margin: 0.001 # margin for ranking loss on candidate summaries
gold_margin: 0 # margin for ranking loss on gold summaries
gold_weight: :0 # weight for ranking loss on gold summaries
mle_weight: 1 # weight for mle loss on gold summaries
rank_weight: 1 # weight for ranking loss on candidate summaries
model_type: facebook/bart-large-cnn # model type
warmup_steps: 10000 # warmup steps
normalize: true # normalize predicited likelihood
grad_norm: 0 # gradient norm
seed: 970903 # random seed
no_gold: false # whether to use gold summaries
pretrained: # pretrained model path
max_lr: 2e-3 # max learning rate (* 1e-2)
scale: 1 # scale of ranking loss
score_mode: log # use log-likelihood for ranking loss
datatype: diverse # data type
dataset: cnndm # dataset
max_summ_len: 120 # max length of summary
max_candidates: 16 # max number of candidate summaries
smooth: 0.1 # label smoothing
max_doc_len: 1024 # total length of source article
length_penalty: 2.0 # length penalty
do_generate: true # whether to generate summaries during evaluation
gen_max_len: 140 # max length of generated summaries
gen_min_len: 55 # min length of generated summaries
is_pegasus: false # whether to use Pegasus as the baseline model
is_t5: false # whether to use T5 as baseline model. is_pegasus and is_t5 cannot be True at the same time
adding: 0 # used for numerical stability
eval_interval: 1000 # evaluation intervals
num_beams: 4 # number of beams for beam search
fp16: false
train_split: train # name of the directory containing train data split
val_split: val # name of the directory containing val data split
